---
title: "Classification and Regression"
author: "A00277360 | Fredrick J. Sigalla"
date: "18/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages(c("psych","gmodels", "class", "C50"), repos="http://cran.rstudio.com/", INSTALL_opts = c("--html","--no-multiarch","--no-test-load","--no-lock"))
```
## **Packages used**
On top of Base R packages, the following packages must be installed before running the script <br/>
  + class package <br/>
  + gmodels package <br/>
  + dplyr package <br/>
  + C50 package <br/>
  + rmarkdown package <br/>
  + psych package <br/>

## **Custom functions**
  + f_z_score standardization function : - coverts numeric vector values into normal by using z-score value <br/>
  + f_normalize function : - rescale numeric vectors values into  normal form of 0 to 1 scale <br/>
  + f_age_range function : - converts age numeric values into range of values character form <br />
  + f_num_children function : - converts numeric number of children into range of values character form <br /> 

```{r}
f_z_score_standardization <- function(x)
{	
  x_standardized <- (x - mean(x))/sd(x)
  return(x_standardized)
}

f_normalize <- function(x) 
{
  x_normalized <- (x - min(x))/diff(range(x))
  return(x_normalized)
}

f_age_range <- function(x)
{
    if(x >= 1 && x <= 14){
    return("Age_Btn_1_14_Incl")
  }
  else if(x >= 15 && x <= 28){
    return("Age_Btn_15_28_Incl")
  }
  else if(x >= 29 && x <= 42){
    return("Age_Btn_29_42_Incl")
  }
  else if(x >= 43 && x <= 56){
    return("Age_Btn_43_56_Incl")
  }
}

f_num_children <- function(x)
{
  if(x >= 0 && x <= 3){
    return("C_Btn_0_3_Incl")
  }
  else if(x >= 4 && x <= 7){
    return("C_Btn_4_7_Incl")
  }
  else if(x >= 8 && x <= 11){
    return("C_Btn_8_11_Incl")
  }
  else if(x >= 12 && x <= 15){
    return("C_Btn_12_15_Incl")
  }
  else if(x >= 16){
    return("C_16_Above_Incl")
  }
}
```

## **Data sets**
The two data sets that were used for the classification and regression tasks include:<br/>

### **Contraceptive Methods Data Set**
This dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The samples are married women who were either not pregnant or do not know if they were at the time of interview. 
The problem is to predict the current contraceptive method choice (no use, long-term methods, or short-term methods) of a woman based on her demographic and socio-economic characteristics.
```{r}
cmc <- read.csv("data/cmc/cmc_data.csv", stringsAsFactors = FALSE)
str(cmc)
```
### **Combine Cycle Power Plant Data set**
The dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. 
Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly
electrical energy output (EP)  of the plant. A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam 
generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. 
While the Vacuum is colected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.
```{r}
cpp <- read.csv("data/cpp/cpp.csv", stringsAsFactors = TRUE, sep = ";")
str(cpp)
```
## **kNN**
To start with, kNN algorithm was applied on Contraceptive data set to investigate how accurately it classifies the contraceptive method used by the woman. The target feature (the feature to be predicted) is the contraceptive method, which can be one among "No-use", "Long-term" and "Short-term"

```{r}
new_variable_names <- c("Age", "Education", "Husband_Education", "Children", "Religion", "Working", "Husband_Occupation", "Std_Living", "Media_Exposure", "Contraceptive_Method")
names(cmc) <- new_variable_names
str(cmc)

cmc$Contraceptive_Method <- factor(cmc$Contraceptive_Method, levels = c(1, 2, 3), labels = c("No-use", "Long-term", "Short-term"))
str(cmc)

nrow(as.data.frame(complete.cases(cmc)))

summary(cmc[-10])
```
Statiscal summary showed slightly higher range for some numerical features, specifically woman's age and the number of children.<br/>
To check if the features had any outliers, boxplots for both features (age and number of children) were generated
```{r}
boxplot(cmc$Age, main = "Box-Plot of Women Age", ylab = "Age in Years")
boxplot(cmc$Age, main = "Box-Plot - No. Children", ylab = "Number children")
```
<br/>Despite the huge range, the boxplots didn't show to be any outliers in those two features.<br/><br/>

Summary results also showed huge variabiity of statistical valuess. For kNN to work best, all numerical values were normalize by using function f_normalize so as they all lie within 0 to 1 scale

```{r}
cmc_normalized <- as.data.frame(lapply(cmc[-10], f_normalize))
summary(cmc_normalized)
str(cmc_normalized)

target_variable <- cmc$Contraceptive_Method
str(target_variable)
summary(target_variable)
```

The data set was splitted into 7:3 ratio, where, 70% of the data was used as training data and 30% was used as testing data test.
```{r}
head(target_variable, 30)
tail(target_variable, 30)
table(target_variable[1:1031])
table(target_variable[1032:1472])

target_variable_train <- target_variable[1:1031]
round(prop.table(table(target_variable_train)) * 100, 2)
target_variable_test <- target_variable[1032:1472]
round(prop.table(table(target_variable_test)) * 100, 2)

cmc_normalized_training <- cmc_normalized[1:1031,]
cmc_normalized_testing <- cmc_normalized[1032:1472,]
```

```{r}
library(class)
target_variable_pred <- knn(train = cmc_normalized_training, 
                            test = cmc_normalized_testing,
                            cl = target_variable_train, k = 32)
```

<br/> Evaluating performance
```{r}
library(gmodels)
CrossTable(x = target_variable_test, y = target_variable_pred, prop.chisq = FALSE)
```

On implementing kNN, the value of k was 32, and this is because, the training data set had almost 1032 examples, so the value of k was obtained from the square root of the number of the examples.

The predicted vector of contraceptive method was evaluated against the actual contraceptive method to check the model's accuracy

+ The model managed to predict 55% (100 out of 182) of the women who do not use any contraceptive method On the other hand, the model managed to predict only 32% (34 out 106) of the women who use long term contracpetive methods. <br/><br/>
+ Lastly, the model managed to predict 48% (74 out of 153) of the women who use short term contraceptive method <br/><br/>
+ Increasing the value of k (number of nearest neihbours), tend to increase the accuracy in predicting number of women do not use any contraceptive method as well as the number of women using short term contraptive method while decreasing accuracy in predicting women using long contraceptive methods. <br/><br/>
+ Therefore, the model can be used to predict the number of women who do not use contraceptive methods as well as those using short term methods, since the accuracy is guaranteed to be greate than almost 50%. As much as this seems to be relatively low, BUT for an experimet involving human behaviour, this is fairly a good performance.

## **Decision Tree**
Decision Tree with a C5.0 Implementation was used to build a model to perform classification on the same data set (contraceptive methods)

```{r}
str(cmc)
```
<br/><br/> Converting numeric values of featurs Age and Number of Children into range of values in character format (Decision Tree doesnt work well with numeric values)
```{r}
cmc$Age <- as.character(lapply(cmc$Age, f_age_range))
cmc$Children <- as.character(lapply(cmc$Children, f_num_children))
str(cmc)
```
<br/>All categorical features were also converted into factors for the same reason, Decision Tree works best with factors

```{r}
cmc$Education <- factor(cmc$Education, levels = c(1, 2, 3, 4), labels = c("low", "high", "high", "high"))
cmc$Husband_Education <- factor(cmc$Husband_Education, levels = c(1, 2, 3, 4), labels = c("low", "high", "high", "high"))
cmc$Religion <- factor(cmc$Religion, levels = c(0, 1), labels = c("Non-Islam", "Islam"))
cmc$Working <- factor(cmc$Working, levels = c(0, 1), labels = c("Yes", "No"))
cmc$Husband_Occupation <- factor(cmc$Husband_Occupation, levels = c(1, 2, 3, 4), labels = c("1", "2", "3", "4"))
cmc$Std_Living <- factor(cmc$Std_Living, levels = c(1, 2, 3, 4), labels = c("low", "high", "high", "high"))
cmc$Media_Exposure <- factor(cmc$Media_Exposure, levels = c(0, 1), labels = c("Good", "Not good"))
```
For decision tree, the data set was splitted in 9:1 ratio, with 90% of the data being used for training the model and the remaining 10% was used for testing the model
```{r}
round(prop.table(table(cmc$Contraceptive_Method)) * 100, 2)
cmc_dt_train <- cmc[1:1325,]
str(cmc_dt_train)

cmc_dt_test <- cmc[1326:1472,]
str(cmc_dt_test)
    
round(prop.table(table(cmc_dt_train$Contraceptive_Method)) * 100,2)
round(prop.table(table(cmc_dt_test$Contraceptive_Method)) * 100,2)
```
From the above operation, it was observed that, the ration of the target variable was not that fair, therefore, the data was randomised first before being splitted again into 9:1 ration, training and testing data set respectively

```{r}
set.seed(12345)
cmc_rand <- cmc[order(runif(1472)),]
str(cmc_rand)

round(prop.table(table(cmc_rand$Contraceptive_Method)) * 100,2)

cmc_rand_train <- cmc_rand[1:1325,]
str(cmc_rand_train)

cmc_rand_test <- cmc_rand[1326:1472,]
str(cmc_rand_test)
```
<br/> After randomization, the split of the target variable between training and testing data set was reasonably fair
```{r}
round(prop.table(table(cmc_rand_test$Contraceptive_Method)) * 100, 2)
round(prop.table(table(cmc_rand_train$Contraceptive_Method)) * 100, 2)
```

Using C5.0 Decision Tree Implementation to build the model
```{r}
library(C50)
cmc_dt_model <- C5.0(cmc_rand_train[-10], cmc_rand_train$Contraceptive_Method)
summary(cmc_dt_model)
```

<br/> Evaluating the model by using the testing data set
```{r}
cmc_dt_pred <- predict(cmc_dt_model, cmc_rand_test[-10]) 

CrossTable(cmc_rand_test$Contraceptive_Method, cmc_dt_pred, prop.chisq = FALSE, 
           prop.c = FALSE, prop.r = FALSE, dnn = c("Actual CMC", "Predicted CMC"))
```

<br/> + The model is 60% accurate in predicting women who don't use any contraceptive method.It predicted 40 out of 67 women who do not use any contraceptive method.<br/><br/>
+ The model is only 32% accurate in predicting women using long term contraceptive method. <br/><br/>
+ Only 8 out of 28 women using long term methods were correctly predicted. <br/><br/>
+ The model is 50% accurate in predicting women using short term contraceptive methods. 26 out of 52 women using short term method were correctly predicted.<br/><br/>

**Note:** Even after boosting was applied, there was no change in the accuracy of the model.

## **Multiple Regression**
<br/>For multiple regress, the data set for combined cycle power plant was used, where the objective was to create a model that can predict the amount of electrical energy produced basing on othe four independent features namely ambient temperature, ambient pressure, exhaust vacuum and relative humidity.

```{r}
str(cpp)
```
<br/> Data features were renamed to more intutiive names for easy interpretation
```{r}
features <- c("Temperature", "Vacuum", "Pressure", "Humidity", "Energy")
colnames(cpp)
colnames(cpp) <- features
colnames(cpp)
str(cpp)
```
<br/> Data set didn't have any missing value as shown below
```{r}
cpp[rowSums(is.na(cpp)) > 0, ]
```
<br/> Statistical summary
```{r}
summary(cpp)
summary(cpp$Energy)
```
<br/> Checking for outliers as well as confirming the dependent variable is normally distributed. (Linear Regression assume the dependent variable is normally distributed)
```{r}
boxplot(cpp$Energy, main = "Boxplot - Cycle Power Plant", ylab = "Electrical Energy")
hist(cpp$Energy, main = "Histogram - Cycle Power Plant", xlab = "Electrical Energy")
```
<br/> The diagrms above show that, the data didn't have any outliers (boxplot), and it was fairly normally distributed (histogram) though slight right skewed.

<br/><br/> Correlation among independent variables and dependent variable was explored to see how each variable is related to one another
```{r}
library(psych)
pairs.panels(cpp[c('Temperature','Vacuum','Pressure','Humidity','Energy')])
```
<br/><br/>**The key findings from the above Scatterplot Matrix include the follow:** <br/>
+ There is a very strong negative correlation between indepedent variables Ambient Temperature and Exhaust vacuum with dependent variable Electrical energy (-0.95 and -0.87 respectively). This means that, everytime ambient temperature and exhaust vacuum rise, then there is a decrease in electrical energy.<br/><br/>
+ On the other hand, the other two independent variables - ambient pressure and relative humidity, have a fair positive correlation with the dependent variable - electrical energy (0.52 and 0.39 respectively). This means whenever there is an increase in ambient pressure and relative humidity, there is always an increase in electrical energy being produced at the power plant.
<br/><br/>

Training the model
```{r}
cpp_model <- lm(cpp$Energy ~ ., data = cpp)
cpp_model
summary(cpp$Energy)
```
<br/>
The model shows that, when all the independent variables are zero, the amout of electrical energy produced is approximately 454.6 Watts, which is actually greater than the mean energy prodduced. Therefore, with maximum energy produced being 495.8, this mean the independent variables do have an effect in maximizing the amount of energy produced.<br/><br/>

Evaluating the model
```{r}
summary(cpp_model)
```
**From the model evaluation statistics:** <br/>
+ Maximum Residual = 17.778, show that, there is at least one value which was under predicted by 17.778. This is actually not that bad, it's fair amount error (redidual is the difference between the true value and the predicted value)<br/><br/>
+ All four independent variable (Temperature, Humidity, Pressure and Exhaust Vacuum) have significance code:0, this means,they all have huge influence in predicting the dependent variable (Energy)

## **Reflection**
The very first observation from this analysis, specifically on classification task, different algorithms can produce different level of accuracy on the same data. Decison Tree was more accurate in classifying the contraceptive methods compared to kNN.<br/><br/>
However, the choice of which algorithm to use, hugely depends on the objective of doing the analysis. Despite the fact that, overall, Decision Tree produced more accurate classification, kNN on the other hand was more accurately in classifying short term contraceptive methods than decision tree. Therefore, if short term contraceptive methods was the main parameter of interest, kNN would be a better algorithm for this task compared to Decisioin Tree even if overall it had less accurate classification results.<br/><br/>

Analysis of Combined Cycle Power Plant showed that, depending with the relevance of the problem at hand, occassionaly, the dependent variable can have a significant results even when the indpendendent variables are all insignificant (0). This can be proved by the fact that, the coefficient of Electrical Energy was almost equal to the mean energy when all other independent factors (ambient temperature, ambient pressure, exhaust vacuum and relative humidity) were zero. Though this does not mean the independent variable does not have effect on the dependent variables, as the maximum electrical energy (with independnet variables at different non-zero value) was greater than when they were at zero.<br/><br/>

Main challenges faced during analysis can be categorised into two:
+ Environment (analysis platform) challenges
+ Analytical Problem specific challenges<br/><br/>

#### **Environment challenges** <br/>
+ Occasionally, some of the non Base R packages like C50, class and gmodels could be masked from the rmarkdown. So even if the packages are well installed and loaded, rmarkdwon gave a "package not installed" error. This was resolved by installing the package direct from rmarkdown by specifyfing the CRAN source.<br/><br/>

+ Another environment (platform) challenges was again errors while runnign rmarkdwon. This time the error was specifically caused by "psych" package which is essential for plotting Scatterplot Matrix. Everytime an install was run, there folder "00LOCK" was being created and thus halting the execution of the whole rmarkdown file. This error was resolved by occassional manually deleting this folder as well as reloading the RStudio environment.

### **Analytical Problem specific challenges** <br/>
+ While performing analysis using kNN, in the course of improving accuracy by varying the value of k, there was no uniform pattern, the accuracy would change from better to worse and vice versa while varying the value k without any trend, therefore it was impossible to tell if its the increase or the decrease of value of k that would increase model's accuracy. This challenge was solve by using the conventional method of adopting the square root of the number of data set examples as the value of k. <br/><br/>

+ The other challenge experinced was unfair distribution ration of target variables after splitting the data into training and testing data sets. This was resolved by randomising/reshufling the original data set before implementing the split.<br/><br/>

+ Lastly, huge difference in feature values and format was also a challenge. As majority of the learning task require data to be in a certain format. This was solved by converting data into approprate format as well as applying standardization and z-score normalization.

